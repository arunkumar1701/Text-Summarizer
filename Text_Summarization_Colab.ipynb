{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# End-to-End Text Summarization on Google Colab\n",
                "\n",
                "This notebook implements the training and evaluation of the Text Summarization project using the SAMSUM dataset. \n",
                "**Make sure to enable GPU Runtime: Runtime > Change runtime type > T4 GPU**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.57.3)\n",
                        "Requirement already satisfied: datasets in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.4.2)\n",
                        "Requirement already satisfied: rouge_score in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.2)\n",
                        "Requirement already satisfied: deep-translator in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.11.4)\n",
                        "Requirement already satisfied: accelerate in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12.0)\n",
                        "Requirement already satisfied: filelock in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.20.1)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.6)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (22.0.0)\n",
                        "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.4.0)\n",
                        "Requirement already satisfied: pandas in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.3.3)\n",
                        "Requirement already satisfied: httpx<1.0.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.28.1)\n",
                        "Requirement already satisfied: xxhash in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.18)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
                        "Requirement already satisfied: anyio in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
                        "Requirement already satisfied: idna in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
                        "Requirement already satisfied: absl-py in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (2.3.1)\n",
                        "Requirement already satisfied: nltk in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (3.9.2)\n",
                        "Requirement already satisfied: six>=1.14.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge_score) (1.17.0)\n",
                        "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deep-translator) (4.14.3)\n",
                        "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8.1)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.6.2)\n",
                        "Requirement already satisfied: psutil in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (7.2.0)\n",
                        "Requirement already satisfied: torch>=2.0.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (2.9.1)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
                        "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
                        "Requirement already satisfied: jinja2 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
                        "Requirement already satisfied: colorama in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
                        "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
                        "Requirement already satisfied: click in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge_score) (8.3.1)\n",
                        "Requirement already satisfied: joblib in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge_score) (1.5.3)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arun kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.3)\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers datasets rouge_score deep-translator accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'torch'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from collections import Counter\n",
                "from datasets import load_dataset\n",
                "import logging\n",
                "import pickle\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "# Set seed\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "torch.cuda.manual_seed(SEED)\n",
                "torch.backends.cudnn.deterministic = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. Data Ingestion & Processing ---\n",
                "\n",
                "def clean_text(text):\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    text = re.sub(r'http\\S+', '', text)\n",
                "    # Remove emojis and special characters (keep alphanumeric and basic punctuation)\n",
                "    text = re.sub(r'[^\\w\\s.,!?\\']', '', text)\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "logger.info(\"Downloading Dataset...\")\n",
                "dataset = load_dataset('knkarthick/samsum')\n",
                "\n",
                "train_texts = [clean_text(t) for t in dataset['train']['dialogue']]\n",
                "train_summaries = [clean_text(t) for t in dataset['train']['summary']]\n",
                "val_texts = [clean_text(t) for t in dataset['validation']['dialogue']]\n",
                "val_summaries = [clean_text(t) for t in dataset['validation']['summary']]\n",
                "test_texts = [clean_text(t) for t in dataset['test']['dialogue']]\n",
                "test_summaries = [clean_text(t) for t in dataset['test']['summary']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'logger' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m         indices = indices[:max_len-\u001b[32m2\u001b[39m]\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33m\"\u001b[39m\u001b[33mBuilding Vocabulary...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m vocab = Vocabulary()\n\u001b[32m     49\u001b[39m vocab.build_vocabulary(train_texts + train_summaries)\n",
                        "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
                    ]
                }
            ],
            "source": [
                "# --- 2. Vocabulary & Dataset ---\n",
                "\n",
                "MAX_VOCAB_SIZE = 20000\n",
                "MAX_LEN_TEXT = 200\n",
                "MAX_LEN_SUMMARY = 50\n",
                "BATCH_SIZE = 64  # Increased for Colab GPU\n",
                "EMBEDDING_DIM = 256\n",
                "HIDDEN_DIM = 512\n",
                "\n",
                "SOS_TOKEN = '<SOS>'\n",
                "EOS_TOKEN = '<EOS>'\n",
                "PAD_TOKEN = '<PAD>'\n",
                "UNK_TOKEN = '<UNK>'\n",
                "\n",
                "class Vocabulary:\n",
                "    def __init__(self):\n",
                "        self.word2index = {}\n",
                "        self.index2word = {}\n",
                "        self.vocab_size = 0\n",
                "\n",
                "    def build_vocabulary(self, sentence_list):\n",
                "        counter = Counter()\n",
                "        for sentence in sentence_list:\n",
                "            if isinstance(sentence, str):\n",
                "                counter.update(sentence.split())\n",
                "        \n",
                "        self.add_word(PAD_TOKEN)\n",
                "        self.add_word(SOS_TOKEN)\n",
                "        self.add_word(EOS_TOKEN)\n",
                "        self.add_word(UNK_TOKEN)\n",
                "        \n",
                "        for word, _ in counter.most_common(MAX_VOCAB_SIZE):\n",
                "            self.add_word(word)\n",
                "\n",
                "    def add_word(self, word):\n",
                "        if word not in self.word2index:\n",
                "            self.word2index[word] = self.vocab_size\n",
                "            self.index2word[self.vocab_size] = word\n",
                "            self.vocab_size += 1\n",
                "\n",
                "    def text_to_indices(self, text, max_len):\n",
                "        tokens = text.split() if isinstance(text, str) else []\n",
                "        indices = [self.word2index.get(token, self.word2index[UNK_TOKEN]) for token in tokens]\n",
                "        indices = indices[:max_len-2]\n",
                "        return indices\n",
                "\n",
                "logger.info(\"Building Vocabulary...\")\n",
                "vocab = Vocabulary()\n",
                "vocab.build_vocabulary(train_texts + train_summaries)\n",
                "logger.info(f\"Vocab Size: {vocab.vocab_size}\")\n",
                "\n",
                "class SumDataset(Dataset):\n",
                "    def __init__(self, texts, summaries, vocab):\n",
                "        self.texts = texts\n",
                "        self.summaries = summaries\n",
                "        self.vocab = vocab\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.texts)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        text = self.texts[idx]\n",
                "        summary = self.summaries[idx]\n",
                "        \n",
                "        src = self.vocab.text_to_indices(text, MAX_LEN_TEXT)\n",
                "        src = src + [self.vocab.word2index[PAD_TOKEN]] * (MAX_LEN_TEXT - len(src))\n",
                "        \n",
                "        trg = [self.vocab.word2index[SOS_TOKEN]] + self.vocab.text_to_indices(summary, MAX_LEN_SUMMARY)\n",
                "        trg_y = self.vocab.text_to_indices(summary, MAX_LEN_SUMMARY) + [self.vocab.word2index[EOS_TOKEN]]\n",
                "        \n",
                "        padding = [self.vocab.word2index[PAD_TOKEN]] * (MAX_LEN_SUMMARY - len(trg) + 1)\n",
                "        trg = (trg + padding)[:MAX_LEN_SUMMARY]\n",
                "        padding_y = [self.vocab.word2index[PAD_TOKEN]] * (MAX_LEN_SUMMARY - len(trg_y) + 1)\n",
                "        trg_y = (trg_y + padding_y)[:MAX_LEN_SUMMARY]\n",
                "        \n",
                "        return torch.tensor(src), torch.tensor(trg), torch.tensor(trg_y)\n",
                "\n",
                "train_dataset = SumDataset(train_texts, train_summaries, vocab)\n",
                "val_dataset = SumDataset(val_texts, val_summaries, vocab)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. Model Architecture (Custom LSTM) ---\n",
                "\n",
                "class Encoder(nn.Module):\n",
                "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
                "        self.rnn = nn.LSTM(emb_dim, hid_dim, bidirectional=True)\n",
                "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, src):\n",
                "        embedded = self.dropout(self.embedding(src))\n",
                "        outputs, (hidden, cell) = self.rnn(embedded)\n",
                "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
                "        return outputs, hidden, cell\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    def __init__(self, hid_dim):\n",
                "        super().__init__()\n",
                "        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)\n",
                "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
                "\n",
                "    def forward(self, hidden, encoder_outputs):\n",
                "        batch_size = encoder_outputs.shape[1]\n",
                "        src_len = encoder_outputs.shape[0]\n",
                "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
                "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
                "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
                "        attention = self.v(energy).squeeze(2)\n",
                "        return torch.softmax(attention, dim=1)\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
                "        super().__init__()\n",
                "        self.output_dim = output_dim\n",
                "        self.attention = attention\n",
                "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
                "        self.rnn = nn.LSTM((hid_dim * 2) + emb_dim, hid_dim)\n",
                "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, input, hidden, cell, encoder_outputs):\n",
                "        input = input.unsqueeze(0)\n",
                "        embedded = self.dropout(self.embedding(input))\n",
                "        a = self.attention(hidden, encoder_outputs)\n",
                "        a = a.unsqueeze(1)\n",
                "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
                "        weighted = torch.bmm(a, encoder_outputs)\n",
                "        weighted = weighted.permute(1, 0, 2)\n",
                "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
                "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell))\n",
                "        embedded = embedded.squeeze(0)\n",
                "        output = output.squeeze(0)\n",
                "        weighted = weighted.squeeze(0)\n",
                "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
                "        return prediction, hidden.squeeze(0), cell\n",
                "\n",
                "class Seq2Seq(nn.Module):\n",
                "    def __init__(self, encoder, decoder, device):\n",
                "        super().__init__()\n",
                "        self.encoder = encoder\n",
                "        self.decoder = decoder\n",
                "        self.device = device\n",
                "        \n",
                "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
                "        src = src.permute(1, 0)\n",
                "        trg = trg.permute(1, 0)\n",
                "        batch_size = src.shape[1]\n",
                "        trg_len = trg.shape[0]\n",
                "        trg_vocab_size = self.decoder.output_dim\n",
                "        \n",
                "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
                "        encoder_outputs, hidden, cell = self.encoder(src)\n",
                "        \n",
                "        # Correctly handle bi-directional cell state summation for uni-directional decoder\n",
                "        cell_fwd = cell[0]\n",
                "        cell_bwd = cell[1]\n",
                "        cell = (cell_fwd + cell_bwd).unsqueeze(0)\n",
                "        \n",
                "        input = trg[0,:] \n",
                "        for t in range(1, trg_len):\n",
                "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
                "            outputs[t] = output\n",
                "            teacher_force = random.random() < teacher_forcing_ratio\n",
                "            top1 = output.argmax(1)\n",
                "            input = trg[t] if teacher_force else top1\n",
                "        return outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "# --- 4. Training Custom Model --- \n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "logger.info(f\"Using Device: {DEVICE}\")\n",
                "\n",
                "enc = Encoder(vocab.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, 0.5)\n",
                "attn = Attention(HIDDEN_DIM)\n",
                "dec = Decoder(vocab.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, 0.5, attn)\n",
                "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2index[PAD_TOKEN])\n",
                "\n",
                "EPOCHS = 7 # As requested\n",
                "PATIENCE = 3\n",
                "best_val_loss = float('inf')\n",
                "patience_counter = 0\n",
                "\n",
                "print(\"Starting Custom Model Training...\")\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
                "    \n",
                "    for i, (src, trg, trg_y) in enumerate(progress_bar):\n",
                "        src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        output = model(src, trg)\n",
                "        \n",
                "        output_dim = output.shape[-1]\n",
                "        output = output[1:].view(-1, output_dim)\n",
                "        trg_y = trg_y.permute(1,0)[1:].reshape(-1)\n",
                "        \n",
                "        loss = criterion(output, trg_y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "        \n",
                "        progress_bar.set_postfix(loss=loss.item())\n",
                "            \n",
                "    avg_train_loss = epoch_loss/len(train_loader)\n",
                "    print(f\"\\nEpoch {epoch+1} Train Loss: {avg_train_loss:.4f}\")\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for i, (src, trg, trg_y) in enumerate(val_loader):\n",
                "            src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)\n",
                "            output = model(src, trg, 0)\n",
                "            output_dim = output.shape[-1]\n",
                "            output = output[1:].view(-1, output_dim)\n",
                "            trg_y = trg_y.permute(1,0)[1:].reshape(-1)\n",
                "            loss = criterion(output, trg_y)\n",
                "            val_loss += loss.item()\n",
                "            \n",
                "    avg_val_loss = val_loss/len(val_loader)\n",
                "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
                "    \n",
                "    if avg_val_loss < best_val_loss:\n",
                "        best_val_loss = avg_val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save(model.state_dict(), \"custom_model.pth\")\n",
                "        print(\"Validation Loss Improved - Model Saved!\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        print(f\"Validation Loss did not improve. Patience: {patience_counter}/{PATIENCE}\")\n",
                "        \n",
                "    if patience_counter >= PATIENCE:\n",
                "        print(\"Early Stopping Triggered.\")\n",
                "        break\n",
                "\n",
                "with open(\"vocab.pkl\", \"wb\") as f:\n",
                "    pickle.dump(vocab, f)\n",
                "print(\"Custom Model Training Completed.\")\n",
                "\"\"\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'transformers'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 5. Pegasus (Transformers) Training --- \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PegasusForConditionalGeneration, PegasusTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Pegasus Training... (This may take memory)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mgoogle/pegasus-cnn_dailymail\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
                    ]
                }
            ],
            "source": [
                "# --- 5. Pegasus (Transformers) Training --- \n",
                "\n",
                "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "\n",
                "print(\"\\nStarting Pegasus Training... (This may take memory)\")\n",
                "\n",
                "model_name = \"google/pegasus-cnn_dailymail\"\n",
                "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
                "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n",
                "\n",
                "def preprocess_function(examples):\n",
                "    inputs = examples[\"dialogue\"]\n",
                "    targets = examples[\"summary\"]\n",
                "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
                "    with tokenizer.as_target_tokenizer():\n",
                "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
                "\n",
                "args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"pegasus-samsum-model\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    learning_rate=5e-5,\n",
                "    per_device_train_batch_size=4, # Small batch for Colab depending on GPU mem\n",
                "    per_device_eval_batch_size=4,\n",
                "    weight_decay=0.01,\n",
                "    save_total_limit=1,\n",
                "    num_train_epochs=3,\n",
                "    predict_with_generate=True,\n",
                "    fp16=True, # Enable mixed precision for T4\n",
                "    logging_steps=10,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=pegasus_model)\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=pegasus_model,\n",
                "    args=args,\n",
                "    train_dataset=tokenized_dataset[\"train\"],\n",
                "    eval_dataset=tokenized_dataset[\"validation\"],\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=tokenizer,\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "pegasus_model.save_pretrained(\"pegasus-samsum-model\")\n",
                "tokenizer.save_pretrained(\"pegasus-samsum-model\")\n",
                "print(\"Pegasus Training Completed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 6. Evaluation & Comparison --- \n",
                "\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# Load Custom Model\n",
                "custom_model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
                "custom_model.load_state_dict(torch.load(\"custom_model.pth\"))\n",
                "custom_model.eval()\n",
                "\n",
                "# Evaluation Function\n",
                "def custom_predict(model, vocab, text):\n",
                "    model.eval()\n",
                "    tokens = text.split()\n",
                "    indices = [vocab.word2index.get(t, vocab.word2index['<UNK>']) for t in tokens]\n",
                "    src_tensor = torch.tensor(indices).unsqueeze(1).to(DEVICE)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
                "        \n",
                "        # Handle cell for bidirectional encoder -> unidirectional decoder\n",
                "        cell_fwd = cell[0]\n",
                "        cell_bwd = cell[1]\n",
                "        cell = (cell_fwd + cell_bwd).unsqueeze(0)\n",
                "    \n",
                "    trg_indexes = [vocab.word2index.get('<SOS>')]\n",
                "    for i in range(50):\n",
                "        trg_tensor = torch.tensor([trg_indexes[-1]]).to(DEVICE)\n",
                "        output, hidden, cell = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
                "        pred_token = output.argmax(1).item()\n",
                "        trg_indexes.append(pred_token)\n",
                "        if pred_token == vocab.word2index.get('<EOS>'):\n",
                "            break\n",
                "    trg_tokens = [vocab.index2word.get(i, \"\") for i in trg_indexes]\n",
                "    return \" \".join(trg_tokens[1:-1])\n",
                "\n",
                "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
                "results = []\n",
                "\n",
                "print(\"Evaluating on Test Set (First 10 samples for demo)...\")\n",
                "test_subset = dataset['test'].select(range(10))\n",
                "\n",
                "for example in tqdm(test_subset):\n",
                "    dialogue = clean_text(example['dialogue'])\n",
                "    reference = clean_text(example['summary'])\n",
                "    \n",
                "    # Custom Model\n",
                "    custom_summ = custom_predict(custom_model, vocab, dialogue)\n",
                "    \n",
                "    # Pegasus\n",
                "    inputs = tokenizer(dialogue, max_length=512, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
                "    summary_ids = pegasus_model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, length_penalty=2.0)\n",
                "    pegasus_summ = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
                "    \n",
                "    c_scores = scorer.score(reference, custom_summ)\n",
                "    p_scores = scorer.score(reference, pegasus_summ)\n",
                "    \n",
                "    results.append({\n",
                "        \"dialogue\": dialogue[:100] + \"...\",\n",
                "        \"reference\": reference,\n",
                "        \"custom\": custom_summ,\n",
                "        \"pegasus\": pegasus_summ,\n",
                "        \"custom_r1\": c_scores['rouge1'].fmeasure,\n",
                "        \"pegasus_r1\": p_scores['rouge1'].fmeasure\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(df[['custom_r1', 'pegasus_r1']].mean())\n",
                "df.to_csv(\"comparison_results.csv\")\n",
                "print(\"Saved results to comparison_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
