
# -*- coding: utf-8 -*-
"""Text_Summarization_LargeData_Resumable.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# Resumable Text Summarization Training (CNN/DailyMail)

Use this script to continue training from a saved checkpoint (e.g., Epoch 4) 
if your session crashed.
"""

# !pip install transformers datasets rouge_score tokenizers accelerate torch pandas

import os
import math
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
import logging
import random
from tqdm import tqdm

# --- RESUME CONFIGURATION ---
# Set this to the epoch number you finished (e.g., 4). 
# The script will start training from Epoch 5.
LAST_FINISHED_EPOCH = 9 
CHECKPOINT_PATH = f"transformer_cnndm_epoch_{LAST_FINISHED_EPOCH}.pth" # e.g., transformer_cnndm_epoch_4.pth

# If using Google Drive, uncomment and adjust:
# from google.colab import drive
# drive.mount('/content/drive')
# CHECKPOINT_PATH = f"/content/drive/My Drive/transformer_cnndm_epoch_{LAST_FINISHED_EPOCH}.pth"

# ---------------------------

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set Device
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using Device: {DEVICE}")

# Set seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# --- 1. Data Ingestion (CNN/DailyMail) ---

print("Downloading CNN/DailyMail Dataset (Version 3.0.0)...")
dataset = load_dataset('cnn_dailymail', '3.0.0')

train_data = dataset['train']
val_data = dataset['validation']
# optimize: reduced subset for val to speed up checks if needed
val_data = val_data.select(range(5000)) 

print(f"Training Samples: {len(train_data)}")

# --- 2. Tokenizer (Load or Train) ---

if not os.path.exists("data_dump"):
    os.makedirs("data_dump")

# We assume you might have lost the tokenizer folder, so we re-train it (it's fast, <1 min)
# If you uploaded 'tokenizer_large', we could load it, but re-training ensures consistency if upload fails.
print("Preparing tokenizer data (subset)...")
with open("data_dump/train_subset.txt", "w", encoding="utf-8") as f:
    for i in range(min(20000, len(train_data))): 
        t = train_data[i]['article']
        s = train_data[i]['highlights']
        f.write(t + "\n" + s + "\n")

print("Training/Loading BPE Tokenizer...")
tokenizer = ByteLevelBPETokenizer()
tokenizer.train(files=["data_dump/train_subset.txt"], vocab_size=30000, min_frequency=2, special_tokens=[
    "<s>", "<pad>", "</s>", "<unk>", "<mask बहन>"
])

SOS_TOKEN_ID = tokenizer.token_to_id("<s>")
EOS_TOKEN_ID = tokenizer.token_to_id("</s>")
PAD_TOKEN_ID = tokenizer.token_to_id("<pad>")

print(f"Vocab Size: {tokenizer.get_vocab_size()}")

# --- 3. Configuration & Dataset ---

# Hyperparameters (MUST MATCH PREVIOUS RUN)
MAX_LEN_TEXT = 512
MAX_LEN_SUMMARY = 128
BATCH_SIZE = 16    
EMB_DIM = 256
N_HEAD = 4
HID_DIM = 512
N_LAYERS = 4
DROPOUT = 0.1
EPOCHS = 10        
LEARNING_RATE = 0.0001
GRAD_CLIP = 1.0
LABEL_SMOOTHING = 0.1

class CNNDMDataset(Dataset):
    def __init__(self, data, tokenizer, max_len_text, max_len_summary):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len_text = max_len_text
        self.max_len_summary = max_len_summary

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = str(item['article'])
        summary = str(item['highlights'])
        
        # Encode
        enc_src = self.tokenizer.encode(text).ids[:self.max_len_text-2]
        enc_trg = self.tokenizer.encode(summary).ids[:self.max_len_summary-1]
        
        src = enc_src + [PAD_TOKEN_ID] * (self.max_len_text - len(enc_src))
        trg = [SOS_TOKEN_ID] + enc_trg
        trg_y = enc_trg + [EOS_TOKEN_ID]
        
        padding = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg))
        trg = (trg + padding)[:self.max_len_summary]
        
        padding_y = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg_y))
        trg_y = (trg_y + padding_y)[:self.max_len_summary]
        
        return torch.tensor(src), torch.tensor(trg), torch.tensor(trg_y)

print("Creating DataLoaders...")
train_dataset = CNNDMDataset(train_data, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)
val_dataset = CNNDMDataset(val_data, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

# --- 4. Transformer Model ---

class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)
        pos_encoding = torch.zeros(max_len, dim_model)
        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)
        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pos_encoding", pos_encoding)
    def forward(self, token_embedding):
        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])

class TransformerModel(nn.Module):
    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p, pad_idx):
        super().__init__()
        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=2000)
        self.embedding = nn.Embedding(num_tokens, dim_model)
        self.transformer = nn.Transformer(
            d_model=dim_model, nhead=num_heads, num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers, dropout=dropout_p, batch_first=True
        )
        self.out = nn.Linear(dim_model, num_tokens)
        self.pad_idx = pad_idx

    def forward(self, src, trg, src_pad_mask=None, trg_pad_mask=None):
        src_emb = self.embedding(src) * math.sqrt(self.positional_encoder.pos_encoding.shape[2])
        trg_emb = self.embedding(trg) * math.sqrt(self.positional_encoder.pos_encoding.shape[2])
        
        src_emb = self.positional_encoder(src_emb.permute(1,0,2)).permute(1,0,2)
        trg_emb = self.positional_encoder(trg_emb.permute(1,0,2)).permute(1,0,2)

        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(DEVICE)
        output = self.transformer(
            src=src_emb, tgt=trg_emb, src_key_padding_mask=src_pad_mask,
            tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask, tgt_mask=trg_mask
        )
        return self.out(output)

    def create_pad_mask(self, matrix, pad_token):
        return (matrix == pad_token)

# --- 5. Load Checkpoint and Train ---

model = TransformerModel(
    num_tokens=tokenizer.get_vocab_size(), dim_model=EMB_DIM, num_heads=N_HEAD, 
    num_encoder_layers=N_LAYERS, num_decoder_layers=N_LAYERS, dropout_p=DROPOUT, pad_idx=PAD_TOKEN_ID
).to(DEVICE)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID, label_smoothing=LABEL_SMOOTHING)

# RESUME LOGIC
if os.path.exists(CHECKPOINT_PATH):
    print(f"Loading checkpoint from {CHECKPOINT_PATH}...")
    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))
    start_epoch = LAST_FINISHED_EPOCH
    print(f"Resuming training from Epoch {start_epoch + 1}")
else:
    print(f"Checkpoint {CHECKPOINT_PATH} not found! Starting from scratch?")
    start_epoch = 0

print("Starting Training...")

for epoch in range(start_epoch, EPOCHS):
    model.train()
    epoch_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")
    
    for i, (src, trg, trg_y) in enumerate(progress_bar):
        src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
        
        src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
        trg_pad_mask = model.create_pad_mask(trg, PAD_TOKEN_ID)
        
        optimizer.zero_grad()
        output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
        
        output = output.reshape(-1, output.shape[-1])
        trg_y = trg_y.reshape(-1)
        
        loss = criterion(output, trg_y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    
    print(f"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}")
    
    # Save checkpoint
    torch.save(model.state_dict(), f"transformer_cnndm_epoch_{epoch+1}.pth")

print("Training Completed.")
