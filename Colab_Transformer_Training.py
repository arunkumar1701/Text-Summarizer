
# -*- coding: utf-8 -*-
"""Text_Summarization_Transformer_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# End-to-End Text Summarization with Transformers (Scratch)

This notebook implements a Transformer model from scratch (using PyTorch `nn.Transformer`) for text summarization on the SAMSUM dataset.
"""

# Install necessary libraries if not present
# !pip install transformers datasets rouge_score deep-translator accelerate torch pandas

import os
import math
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
from datasets import load_dataset
import logging
import pickle
import random
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set Device
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using Device: {DEVICE}")

# Set seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# --- 1. Data Ingestion & Processing ---

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^\w\s.,!?\']', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

print("Downloading Dataset...")
dataset = load_dataset('knkarthick/samsum')

train_texts = [clean_text(t) for t in dataset['train']['dialogue']]
train_summaries = [clean_text(t) for t in dataset['train']['summary']]
val_texts = [clean_text(t) for t in dataset['validation']['dialogue']]
val_summaries = [clean_text(t) for t in dataset['validation']['summary']]
test_texts = [clean_text(t) for t in dataset['test']['dialogue']]
test_summaries = [clean_text(t) for t in dataset['test']['summary']]

# --- 2. Configuration & Vocabulary ---

# Hyperparameters
MAX_VOCAB_SIZE = 20000
MAX_LEN_TEXT = 200
MAX_LEN_SUMMARY = 50
BATCH_SIZE = 32         
EMB_DIM = 256
N_HEAD = 4
HID_DIM = 512
N_LAYERS = 3
DROPOUT = 0.1
EPOCHS = 30
PATIENCE = 5
LEARNING_RATE = 0.0001
GRAD_CLIP = 1.0

SOS_TOKEN = '<SOS>'
EOS_TOKEN = '<EOS>'
PAD_TOKEN = '<PAD>'
UNK_TOKEN = '<UNK>'

class Vocabulary:
    def __init__(self):
        self.word2index = {}
        self.index2word = {}
        self.vocab_size = 0

    def build_vocabulary(self, sentence_list):
        counter = Counter()
        for sentence in sentence_list:
            if isinstance(sentence, str):
                counter.update(sentence.split())
        
        self.add_word(PAD_TOKEN)
        self.add_word(SOS_TOKEN)
        self.add_word(EOS_TOKEN)
        self.add_word(UNK_TOKEN)
        
        for word, _ in counter.most_common(MAX_VOCAB_SIZE):
            self.add_word(word)

    def add_word(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.vocab_size
            self.index2word[self.vocab_size] = word
            self.vocab_size += 1

    def text_to_indices(self, text, max_len):
        tokens = text.split() if isinstance(text, str) else []
        indices = [self.word2index.get(token, self.word2index[UNK_TOKEN]) for token in tokens]
        indices = indices[:max_len-2] 
        return indices

print("Building Vocabulary...")
vocab = Vocabulary()
vocab.build_vocabulary(train_texts + train_summaries)
print(f"Vocab Size: {vocab.vocab_size}")

class SumDataset(Dataset):
    def __init__(self, texts, summaries, vocab, max_len_text, max_len_summary):
        self.texts = texts
        self.summaries = summaries
        self.vocab = vocab
        self.max_len_text = max_len_text
        self.max_len_summary = max_len_summary

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        summary = self.summaries[idx]
        
        src = self.vocab.text_to_indices(text, self.max_len_text)
        src = src + [self.vocab.word2index[PAD_TOKEN]] * (self.max_len_text - len(src))
        
        trg = [self.vocab.word2index[SOS_TOKEN]] + self.vocab.text_to_indices(summary, self.max_len_summary)
        trg_y = self.vocab.text_to_indices(summary, self.max_len_summary) + [self.vocab.word2index[EOS_TOKEN]]
        
        padding = [self.vocab.word2index[PAD_TOKEN]] * (self.max_len_summary - len(trg) + 1)
        trg = (trg + padding)[:self.max_len_summary]
        
        padding_y = [self.vocab.word2index[PAD_TOKEN]] * (self.max_len_summary - len(trg_y) + 1)
        trg_y = (trg_y + padding_y)[:self.max_len_summary]
        
        return torch.tensor(src), torch.tensor(trg), torch.tensor(trg_y)

train_dataset = SumDataset(train_texts, train_summaries, vocab, MAX_LEN_TEXT, MAX_LEN_SUMMARY)
val_dataset = SumDataset(val_texts, val_summaries, vocab, MAX_LEN_TEXT, MAX_LEN_SUMMARY)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# --- 3. Transformer Model ---

class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)
        
        pos_encoding = torch.zeros(max_len, dim_model)
        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)
        
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)
        
        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pos_encoding", pos_encoding)
        
    def forward(self, token_embedding: torch.tensor) -> torch.tensor:
        # Check if we need to resize pe (if sequence is longer than init max_len)
        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])

class TransformerModel(nn.Module):
    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p, pad_idx):
        super().__init__()
        
        self.model_type = "Transformer"
        self.dim_model = dim_model
        
        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=1000)
        self.embedding = nn.Embedding(num_tokens, dim_model)
        
        self.transformer = nn.Transformer(
            d_model=dim_model,
            nhead=num_heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p,
            batch_first=True
        )
        
        self.out = nn.Linear(dim_model, num_tokens)
        self.pad_idx = pad_idx

    def forward(self, src, trg, src_pad_mask=None, trg_pad_mask=None):
        # src: [batch, src_len]
        # trg: [batch, trg_len]
        
        src_emb = self.embedding(src) * math.sqrt(self.dim_model)
        trg_emb = self.embedding(trg) * math.sqrt(self.dim_model)
        
        # Permute for Positional Encoding [seq, batch, dim]
        src_emb = src_emb.permute(1, 0, 2)
        trg_emb = trg_emb.permute(1, 0, 2)
        
        src_emb = self.positional_encoder(src_emb)
        trg_emb = self.positional_encoder(trg_emb)
        
        # Permute back for Transformer [batch, seq, dim]
        src_emb = src_emb.permute(1, 0, 2)
        trg_emb = trg_emb.permute(1, 0, 2)

        # Target Mask (Causal)
        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(DEVICE)
        
        output = self.transformer(
            src=src_emb, 
            tgt=trg_emb, 
            src_key_padding_mask=src_pad_mask,
            tgt_key_padding_mask=trg_pad_mask, 
            memory_key_padding_mask=src_pad_mask,
            tgt_mask=trg_mask
        )
        
        output = self.out(output)
        return output

    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:
        return (matrix == pad_token)

# --- 4. Training ---

model = TransformerModel(
    num_tokens=vocab.vocab_size, 
    dim_model=EMB_DIM, 
    num_heads=N_HEAD, 
    num_encoder_layers=N_LAYERS, 
    num_decoder_layers=N_LAYERS, 
    dropout_p=DROPOUT,
    pad_idx=vocab.word2index[PAD_TOKEN]
).to(DEVICE)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2index[PAD_TOKEN])

print("Starting Training with Transformer...")

best_loss = float('inf')
patience_counter = 0

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")
    
    for i, (src, trg, trg_y) in enumerate(progress_bar):
        src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
        
        src_pad_mask = model.create_pad_mask(src, vocab.word2index[PAD_TOKEN])
        trg_pad_mask = model.create_pad_mask(trg, vocab.word2index[PAD_TOKEN])
        
        optimizer.zero_grad()
        output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
        
        output_dim = output.shape[-1]
        output = output.reshape(-1, output_dim)
        trg_y = trg_y.reshape(-1)
        
        loss = criterion(output, trg_y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    
    avg_train_loss = epoch_loss / len(train_loader)
    
    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for src, trg, trg_y in val_loader:
            src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
            src_pad_mask = model.create_pad_mask(src, vocab.word2index[PAD_TOKEN])
            trg_pad_mask = model.create_pad_mask(trg, vocab.word2index[PAD_TOKEN])
            
            output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
            output = output.reshape(-1, output.shape[-1])
            trg_y = trg_y.reshape(-1)
            loss = criterion(output, trg_y)
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    print(f"\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
    
    # Early Stopping & Saving
    if avg_val_loss < best_loss:
        best_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), "transformer_model.pth")
        print("Model saved!")
    else:
        patience_counter += 1
        print(f"No improvement. Patience: {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print("Early stopping triggered.")
            break

# Save Vocab
with open("vocab.pkl", "wb") as f:
    pickle.dump(vocab, f)

# --- 5. Inference with Beam Search ---

def beam_search_decode(model, src, vocab, max_len=50, beam_size=3):
    model.eval()
    src_pad_mask = model.create_pad_mask(src, vocab.word2index['<PAD>'])
    
    beams = [(0, [vocab.word2index['<SOS>']])] # log prob, sequence
    
    for _ in range(max_len):
        candidates = []
        for score, seq in beams:
            if seq[-1] == vocab.word2index['<EOS>']:
                candidates.append((score, seq))
                continue
            
            trg_tensor = torch.tensor([seq]).to(DEVICE)
            trg_pad_mask = model.create_pad_mask(trg_tensor, vocab.word2index['<PAD>'])
            
            with torch.no_grad():
                output = model(src, trg_tensor, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
            
            logits = output[:, -1, :] 
            probs = torch.log_softmax(logits, dim=1)
            
            topk_probs, topk_indices = torch.topk(probs, beam_size)
            
            for i in range(beam_size):
                token = topk_indices[0][i].item()
                token_prob = topk_probs[0][i].item()
                new_score = score + token_prob
                new_seq = seq + [token]
                candidates.append((new_score, new_seq))
        
        beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_size]
        if all(seq[-1] == vocab.word2index['<EOS>'] for _, seq in beams):
            break
            
    best_seq = beams[0][1]
    tokens = [vocab.index2word[i] for i in best_seq]
    if '<SOS>' in tokens: tokens.remove('<SOS>')
    if '<EOS>' in tokens: tokens.remove('<EOS>')
    return " ".join(tokens)

# Test on a few examples
print("\n--- Testing Model Predictions ---")
for i in range(3):
    text = test_texts[i]
    ref = test_summaries[i]
    
    print(f"\nOriginal: {text}")
    print(f"Reference: {ref}")
    
    indices = [vocab.word2index.get(t, vocab.word2index['<UNK>']) for t in text.split()]
    src_tensor = torch.tensor(indices).unsqueeze(0).to(DEVICE)
    
    pred = beam_search_decode(model, src_tensor, vocab)
    print(f"Prediction: {pred}")
