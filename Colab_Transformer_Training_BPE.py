
# -*- coding: utf-8 -*-
"""Text_Summarization_Transformer_BPE_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# End-to-End Text Summarization with Transformers (BPE Tokenizer)

This notebook implements a Transformer model from scratch using a **BPE Tokenizer**. 
This resolves issues with <UNK> tokens and name confusion.
"""

# !pip install transformers datasets rouge_score tokenizers accelerate torch pandas

import os
import math
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
import logging
import json
import random
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set Device
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using Device: {DEVICE}")

# Set seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# --- 1. Data Ingestion & Processing ---

print("Downloading Dataset...")
dataset = load_dataset('knkarthick/samsum')

def get_text_data(split):
    texts = dataset[split]['dialogue']
    summaries = dataset[split]['summary']
    return texts, summaries

train_texts, train_summaries = get_text_data('train')
val_texts, val_summaries = get_text_data('validation')
test_texts, test_summaries = get_text_data('test')

# --- 2. Train BPE Tokenizer ---

# Save texts to file for tokenizer training
if not os.path.exists("data_dump"):
    os.makedirs("data_dump")

with open("data_dump/train.txt", "w", encoding="utf-8") as f:
    for t, s in zip(train_texts, train_summaries):
        f.write(t + "\n" + s + "\n")

print("Training BPE Tokenizer...")
# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()

# Customize training
tokenizer.train(files=["data_dump/train.txt"], vocab_size=30000, min_frequency=2, special_tokens=[
    "<s>",
    "<pad>",
    "</s>",
    "<unk>",
    "<mask>",
])

# Save tokenizer
if not os.path.exists("tokenizer_bpe"):
    os.makedirs("tokenizer_bpe")
tokenizer.save_model("tokenizer_bpe")

# Wrapper to make it look like our old Vocab class for simplicity, or just use directly
SOS_TOKEN_ID = tokenizer.token_to_id("<s>")
EOS_TOKEN_ID = tokenizer.token_to_id("</s>")
PAD_TOKEN_ID = tokenizer.token_to_id("<pad>")
UNK_TOKEN_ID = tokenizer.token_to_id("<unk>")

print(f"Vocab Size: {tokenizer.get_vocab_size()}")
print(f"PAD ID: {PAD_TOKEN_ID}, SOS ID: {SOS_TOKEN_ID}")

# --- 3. Configuration & Dataset ---

# Hyperparameters
MAX_LEN_TEXT = 256 # Increased len
MAX_LEN_SUMMARY = 64
BATCH_SIZE = 32         
EMB_DIM = 256
N_HEAD = 4
HID_DIM = 512
N_LAYERS = 4 # Deeper
DROPOUT = 0.1
EPOCHS = 35
PATIENCE = 5
LEARNING_RATE = 0.0001
GRAD_CLIP = 1.0
LABEL_SMOOTHING = 0.1

class SumDataset(Dataset):
    def __init__(self, texts, summaries, tokenizer, max_len_text, max_len_summary):
        self.texts = texts
        self.summaries = summaries
        self.tokenizer = tokenizer
        self.max_len_text = max_len_text
        self.max_len_summary = max_len_summary

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        summary = str(self.summaries[idx])
        
        # Encode
        enc_src = self.tokenizer.encode(text).ids
        enc_trg = self.tokenizer.encode(summary).ids
        
        # Truncate
        enc_src = enc_src[:self.max_len_text-2] # Leave room if we added special tokens manually, but verify logic
        enc_trg = enc_trg[:self.max_len_summary-1]
        
        # Add Special Tokens (Using IDs directly)
        src = enc_src # Encoder input usually doesn't need SOS/EOS in all archs, but consistent padding matches
        # Just Pad Source
        src = src + [PAD_TOKEN_ID] * (self.max_len_text - len(src))
        
        # Decoder Input: SOS + Summary
        trg = [SOS_TOKEN_ID] + enc_trg
        # Decoder Target: Summary + EOS
        trg_y = enc_trg + [EOS_TOKEN_ID]
        
        # Pad
        padding = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg))
        trg = (trg + padding)[:self.max_len_summary]
        
        padding_y = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg_y))
        trg_y = (trg_y + padding_y)[:self.max_len_summary]
        
        return torch.tensor(src), torch.tensor(trg), torch.tensor(trg_y)

# Re-init tokenizer to ensure processing logic if needed, but simple encode is enough
train_dataset = SumDataset(train_texts, train_summaries, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)
val_dataset = SumDataset(val_texts, val_summaries, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# --- 4. Transformer Model (Identical structure, handling embedding size) ---

class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)
        
        pos_encoding = torch.zeros(max_len, dim_model)
        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)
        
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)
        
        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pos_encoding", pos_encoding)
        
    def forward(self, token_embedding: torch.tensor) -> torch.tensor:
        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])

class TransformerModel(nn.Module):
    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p, pad_idx):
        super().__init__()
        self.model_type = "Transformer"
        self.dim_model = dim_model
        
        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=1000)
        self.embedding = nn.Embedding(num_tokens, dim_model)
        
        self.transformer = nn.Transformer(
            d_model=dim_model,
            nhead=num_heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p,
            batch_first=True
        )
        
        self.out = nn.Linear(dim_model, num_tokens)
        self.pad_idx = pad_idx

    def forward(self, src, trg, src_pad_mask=None, trg_pad_mask=None):
        src_emb = self.embedding(src) * math.sqrt(self.dim_model)
        trg_emb = self.embedding(trg) * math.sqrt(self.dim_model)
        
        src_emb = src_emb.permute(1, 0, 2)
        trg_emb = trg_emb.permute(1, 0, 2)
        
        src_emb = self.positional_encoder(src_emb)
        trg_emb = self.positional_encoder(trg_emb)
        
        src_emb = src_emb.permute(1, 0, 2)
        trg_emb = trg_emb.permute(1, 0, 2)

        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(DEVICE)
        
        output = self.transformer(
            src=src_emb, 
            tgt=trg_emb, 
            src_key_padding_mask=src_pad_mask,
            tgt_key_padding_mask=trg_pad_mask, 
            memory_key_padding_mask=src_pad_mask,
            tgt_mask=trg_mask
        )
        output = self.out(output)
        return output

    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:
        return (matrix == pad_token)

# --- 5. Training Loop ---

model = TransformerModel(
    num_tokens=tokenizer.get_vocab_size(), 
    dim_model=EMB_DIM, 
    num_heads=N_HEAD, 
    num_encoder_layers=N_LAYERS, 
    num_decoder_layers=N_LAYERS, 
    dropout_p=DROPOUT,
    pad_idx=PAD_TOKEN_ID
).to(DEVICE)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID, label_smoothing=LABEL_SMOOTHING)

print("Starting Training with BPE Transformer...")

best_loss = float('inf')
patience_counter = 0

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")
    
    for i, (src, trg, trg_y) in enumerate(progress_bar):
        src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
        
        src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
        trg_pad_mask = model.create_pad_mask(trg, PAD_TOKEN_ID)
        
        optimizer.zero_grad()
        output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
        
        output_dim = output.shape[-1]
        output = output.reshape(-1, output_dim)
        trg_y = trg_y.reshape(-1)
        
        loss = criterion(output, trg_y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    
    avg_train_loss = epoch_loss / len(train_loader)
    
    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for src, trg, trg_y in val_loader:
            src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
            src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
            trg_pad_mask = model.create_pad_mask(trg, PAD_TOKEN_ID)
            
            output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
            output = output.reshape(-1, output.shape[-1])
            trg_y = trg_y.reshape(-1)
            loss = criterion(output, trg_y)
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    print(f"\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
    
    if avg_val_loss < best_loss:
        best_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), "transformer_bpe_model.pth")
        print("Model saved!")
    else:
        patience_counter += 1
        print(f"No improvement {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            break

# --- 6. Beam Search Inference with Decoder ---

def beam_search_decode(model, src, tokenizer, max_len=50, beam_size=3):
    model.eval()
    src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
    
    beams = [(0, [SOS_TOKEN_ID])] 
    
    for _ in range(max_len):
        candidates = []
        for score, seq in beams:
            if seq[-1] == EOS_TOKEN_ID:
                candidates.append((score, seq))
                continue
            
            trg_tensor = torch.tensor([seq]).to(DEVICE)
            trg_pad_mask = model.create_pad_mask(trg_tensor, PAD_TOKEN_ID)
            
            with torch.no_grad():
                output = model(src, trg_tensor, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
            
            logits = output[:, -1, :] 
            probs = torch.log_softmax(logits, dim=1)
            
            topk_probs, topk_indices = torch.topk(probs, beam_size)
            
            for i in range(beam_size):
                token = topk_indices[0][i].item()
                token_prob = topk_probs[0][i].item()
                new_score = score + token_prob
                new_seq = seq + [token]
                candidates.append((new_score, new_seq))
        
        beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_size]
        if all(seq[-1] == EOS_TOKEN_ID for _, seq in beams):
            break
            
    best_seq = beams[0][1]
    
    # Decode with BPE tokenizer
    decoded = tokenizer.decode(best_seq, skip_special_tokens=True)
    return decoded

print("\n--- Testing Predictions ---")
for i in range(3):
    text = test_texts[i]
    ref = test_summaries[i]
    print(f"\nOriginal: {text}")
    
    enc_ids = tokenizer.encode(text).ids[:MAX_LEN_TEXT-2]
    src_tensor = torch.tensor(enc_ids).unsqueeze(0).to(DEVICE)
    
    pred = beam_search_decode(model, src_tensor, tokenizer)
    print(f"Prediction: {pred}")

import shutil
shutil.make_archive('tokenizer_bpe', 'zip', 'tokenizer_bpe')
print("Zipped tokenizer for download.")
