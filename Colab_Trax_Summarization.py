
# -*- coding: utf-8 -*-
"""Colab_Trax_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# Text Summarization with Trax Transformer

This notebook implements a Transformer model using the **Trax** library, following the reference notebook structure.
It uses a Decoder-Only (Causal) Transformer to perform summarization as a language modeling task.
"""

# !pip install trax jax jaxlib tensorflow-datasets

import os
import sys
import numpy as np
import time
import random

import trax
from trax import layers as tl
from trax.fastmath import numpy as jnp
from trax.supervised import training

# --- 1. Data Ingestion & Preprocessing ---

import tensorflow_datasets as tfds

# Trax handles downloading and streaming TFDS datasets efficiently, but sometimes needs help downloading first.
# We use 'cnn_dailymail' - TFDS will use the latest version (likely 3.4.0)
dataset_name = 'cnn_dailymail'

print(f"Ensuring {dataset_name} is downloaded...")
try:
    # Force download 
    tfds.load(dataset_name, data_dir='data/', download=True)
except Exception as e:
    print(f"Error downloading: {e}")
    print("Trying explicit version 3.0.0...")
    dataset_name = 'cnn_dailymail/3.0.0'
    tfds.load(dataset_name, data_dir='data/', download=True)

train_stream_fn = trax.data.TFDS(dataset_name,
                                 data_dir='data/',
                                 keys=('article', 'highlights'),
                                 train=True)

eval_stream_fn = trax.data.TFDS(dataset_name,
                                data_dir='data/',
                                keys=('article', 'highlights'),
                                train=False)

# Special tokens
PAD = 0
EOS = 1 # End of sentence token
# SEPARATOR implied by structure

def preprocess(stream):
    """Concatenate Article + Summary with EOS and Masking"""
    for (article, summary) in stream:
        # Format: Article + EOS + Padding(0) + Summary + EOS
        # Trax Language Model expects: Input = Target
        # But we mask loss on the Article part so model only learns to output Summary
        
        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])
        
        # Mask: 0 for article (don't train), 1 for summary (train)
        # We mask the EOS and PAD between article and summary too?
        # Reference: "mask -- with 0s at inputs and 1s at targets"
        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1)
        
        # Yield (Input, Target, Mask)
        # In Language Models, Input is usually Target shifted by 1. 
        # Trax TransformerLM handles shifting internally if we pass the same tensor?
        # Actually, standard Trax LM takes (input_tokens, target_tokens, mask)
        yield joint, joint, np.array(mask)

# Input Pipeline
print("Creating Data Pipeline...")

# Vocabulary Handling
VOCAB_DIR = 'vocab_dir'
# Trax is looking for the exact filename, so we include extension
VOCAB_FILE = 'summarize32k.subwords' 
VOCAB_PATH = os.path.join(VOCAB_DIR, VOCAB_FILE)

# The actual filename on disk IS VOCAB_PATH
REAL_VOCAB_PATH = VOCAB_PATH

# For saving (TFDS adds .subwords automatically), we need the base name
VOCAB_BASE = os.path.join(VOCAB_DIR, 'summarize32k')

if not os.path.exists(VOCAB_DIR):
    os.makedirs(VOCAB_DIR)

# Check if vocab exists
if not os.path.exists(REAL_VOCAB_PATH):
    print(f"Vocab file {REAL_VOCAB_PATH} not found. Creating new tokenizer...")
    
    # We need to iterate over the dataset to build the vocab
    # Using 100k samples for maximum robustness (slow but safe)
    print("Building tokenizer from 100,000 samples (patience please)...")
    
    from tqdm import tqdm
    
    def data_generator():
        # Scane more examples to avoid "Token substring not found" error
        count = 0
        limit = 100000 
        ds = trax.data.TFDS(dataset_name, keys=('article', 'highlights'), train=True)()
        
        for item in tqdm(ds, total=limit, desc="Scanning Dataset"):
            if count >= limit:
                break
            yield item[0] # Yield article
            yield item[1] # Yield summary
            count += 1
            
    import datetime
    current_time = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{current_time}] STARTING BPE construction. This process is running in C++ and will be SILENT for 10-20 mins.")
    print("Do not stop the execution. It is working.")

    # Build Subword Tokenizer using TFDS deprecated text encoder (what Trax uses)
    # Approx 32k vocab size
    token_encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
        data_generator(), target_vocab_size=32768
    )
    
    # Save it (save_to_file automatically appends .subwords)
    token_encoder.save_to_file(VOCAB_BASE)
    print(f"Tokenizer saved to {REAL_VOCAB_PATH}")
else:
    print(f"Found existing vocab at {REAL_VOCAB_PATH}")

# --- Safe Tokenization Wrapper ---
# Trax's default Tokenize crashes on unknown chars. We implement a tolerant one.
def SafeTokenize(vocab_file, vocab_dir):
    # Load vocabulary manually
    vocab_path = os.path.join(vocab_dir, vocab_file)
    if not os.path.exists(vocab_path):
        vocab_path += ".subwords"
        
    encoder = tfds.deprecated.text.SubwordTextEncoder.load_from_file(vocab_path[:-9]) # remove .subwords
    
    def safe_encode(s):
        try:
            # Attempt to encode
            return encoder.encode(s)
        except Exception:
            # Fallback: Filter out non-ascii or problematic chars and try again
            # Or just return UNK tokens if we had them. 
            # Simple fallback: encode purely valid ascii, drop others
            s_clean = s.encode('ascii', 'ignore').decode('ascii')
            return encoder.encode(s_clean)

    def tokenize_stream(stream):
        for (article, summary) in stream:
            # Decode bytes to string if needed
            if isinstance(article, bytes): article = article.decode('utf-8')
            if isinstance(summary, bytes): summary = summary.decode('utf-8')
            
            article_ids = safe_encode(article)
            summary_ids = safe_encode(summary)
            
            yield np.array(article_ids), np.array(summary_ids)

    return tokenize_stream

# We inject this BEFORE the preprocess function.
# But Trax serial expects layers. Data pipeline is just functions? 
# Trax data pipelines are just generators.

def valid_generator():
    # We must instantiate the stream and apply tokenization manually
    stream = trax.data.TFDS(dataset_name, keys=('article', 'highlights'), train=True)()
    tokenize_fn = SafeTokenize(VOCAB_FILE, VOCAB_DIR)
    return tokenize_fn(stream)

# Wait... Input pipeline logic in Trax is:
# data -> Tokenize -> Preprocess -> Filter -> Bucket
# We replace `trax.data.Tokenize` with our logical equivalent.

# Reuse the Tokenize logic but safe
safe_tokenizer_fn = SafeTokenize(VOCAB_FILE, VOCAB_DIR) # Use FILE name here, SafeTokenize joins it with DIR

# Redefine pipeline using functional composition
# We can't use trax.data.Serial easily with a custom python generator function in the middle 
# unless we wrap it as a `trax.data.Serial` element or just construct the stream manually.

# Let's construct the stream manually:
train_stream = safe_tokenizer_fn(trax.data.TFDS(dataset_name, keys=('article', 'highlights'), train=True)())
eval_stream = safe_tokenizer_fn(trax.data.TFDS(dataset_name, keys=('article', 'highlights'), train=False)())

# Now apply preprocess and filtering
train_stream = trax.data.Serial(preprocess, trax.data.FilterByLength(2048))(train_stream)
eval_stream = trax.data.Serial(preprocess, trax.data.FilterByLength(2048))(eval_stream)

# Bucketing remains the same
# boundaries =  [128, 256,  512, 1024]
# batch_sizes = [16,    8,    4,    2, 1]

train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(train_stream)
eval_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(eval_stream)


# --- 2. Causal Attention Mechanism (From Scratch) ---

def create_tensor(t):
    return jnp.array(t)

def DotProductAttention(query, key, value, mask):
    """Dot product self-attention."""
    assert query.shape[-1] == key.shape[-1] == value.shape[-1]
    depth = query.shape[-1]
    
    # Q * K^T / sqrt(d)
    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)
    
    if mask is not None:
        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))
    
    # Softmax
    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)
    dots = jnp.exp(dots - logsumexp)
    
    # Dropout (optional, usually skipped in simple inference implementation)
    
    # Attention * V
    attention = jnp.matmul(dots, value)
    return attention

def compute_attention_heads_closure(n_heads, d_head):
    def compute_attention_heads(x):
        batch_size = x.shape[0]
        seqlen = x.shape[1]
        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))
        x = jnp.transpose(x, (0, 2, 1, 3))
        x = jnp.reshape(x, (-1, seqlen, d_head))
        return x
    return compute_attention_heads

def compute_attention_output_closure(n_heads, d_head):
    def compute_attention_output(x):
        seqlen = x.shape[1]
        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))
        x = jnp.transpose(x, (0, 2, 1, 3))
        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))
    return compute_attention_output

def dot_product_self_attention(q, k, v):
    mask_size = q.shape[-2]
    # Causal mask: Lower triangle is True
    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)
    return DotProductAttention(q, k, v, mask)

def CausalAttention(d_feature, n_heads, mode='train'):
    assert d_feature % n_heads == 0
    d_head = d_feature // n_heads
    
    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)
    
    return tl.Serial(
        tl.Branch(
            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # Q
            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # K
            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # V
        ),
        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1),
        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1),
        tl.Dense(d_feature)
    )

# --- 3. Transformer Decoder Block ---

def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):
    causal_attention = CausalAttention(d_model, n_heads=n_heads, mode=mode)
    
    feed_forward = [
        tl.LayerNorm(),
        tl.Dense(d_ff),
        ff_activation(),
        tl.Dropout(rate=dropout, mode=mode),
        tl.Dense(d_model),
        tl.Dropout(rate=dropout, mode=mode)
    ]
    
    return [
        tl.Residual(
            tl.LayerNorm(),
            causal_attention,
            tl.Dropout(rate=dropout, mode=mode)
        ),
        tl.Residual(
            feed_forward
        )
    ]

# --- 4. Transformer Language Model ---

def TransformerLM(vocab_size=33300,
                  d_model=512,
                  d_ff=2048,
                  n_layers=6,
                  n_heads=8,
                  dropout=0.1,
                  max_len=4096,
                  mode='train',
                  ff_activation=tl.Relu):
    
    positional_encoder = [
        tl.Embedding(vocab_size, d_model),
        tl.Dropout(rate=dropout, mode=mode),
        tl.PositionalEncoding(max_len=max_len, mode=mode)
    ]
    
    decoder_blocks = [
        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) 
        for _ in range(n_layers)
    ]
    
    return tl.Serial(
        tl.ShiftRight(1, mode=mode),
        positional_encoder,
        decoder_blocks,
        tl.LayerNorm(),
        tl.Dense(vocab_size),
        tl.LogSoftmax()
    )

# --- 5. Training Loop ---

print("Setting up Training loop...")

output_dir = os.path.expanduser('~/model_trax')

# Clear old checkpoints to prevent shape mismatch errors if architecture changed
# Only do this if you want a fresh start!
if os.path.exists(output_dir):
    print(f"Clearing old checkpoints in {output_dir} to prevent loading errors...")
    import shutil
    shutil.rmtree(output_dir)
os.makedirs(output_dir)

# Learning Rate Schedule
lr_schedule = trax.lr.warmup_and_rsqrt_decay(
    n_warmup_steps=1000, max_value=0.01
)

# Tasks
train_task = training.TrainTask(
    labeled_data=train_batch_stream,
    loss_layer=tl.CrossEntropyLoss(),
    optimizer=trax.optimizers.Adam(0.01),
    lr_schedule=lr_schedule,
    n_steps_per_checkpoint=50  # More frequent updates (every 50 steps)
)

eval_task = training.EvalTask(
    labeled_data=eval_batch_stream,
    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],
    n_eval_batches=10
)

# The Training Loop
training_loop = training.Loop(
    TransformerLM(mode='train'),
    train_task,
    eval_tasks=[eval_task],
    output_dir=output_dir
)

print("Starting Training...")
# Run for 2000 steps with progress bar
total_steps = 2000
steps_per_call = 50

from tqdm import tqdm
for i in tqdm(range(0, total_steps, steps_per_call), desc="Training Progress"):
    training_loop.run(steps_per_call)

print("Training Complete!")

# --- 6. Save & Download ---
# Trax automatically saves checkpoints to `output_dir` (e.g., model.pkl.gz)
# We will zip this folder to make it easy to download from Colab.

print(f"Model checkpoints saved in {output_dir}")

import shutil
zip_filename = "trax_transformer_model"
print(f"Zipping model directory to {zip_filename}.zip...")
shutil.make_archive(zip_filename, 'zip', output_dir)
print(f"Model zipped successfully! Download '{zip_filename}.zip' from the files tab.")

# --- 7. Inference / Generation Skeleton ---

def generate_summary(model, article, vocab_file='summarize32k.subword.subwords', vocab_dir='vocab_dir/'):
    # Load tokenizer and detokenizer logic locally if possible or reuse Trax
    # For generation, we feed [Article, EOS, PAD] and let model predict next tokens
    pass
    # Implementation depends on having valid vocab files locally
