
# -*- coding: utf-8 -*-
"""Text_Summarization_LargeData_Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# Text Summarization from Scratch (Large Dataset: CNN/DailyMail)

This notebook implements a Transformer model trained from scratch on the **CNN/DailyMail** dataset (~300k examples).
Using a large dataset is critical for the model to learn **Attention Mechanisms** correctly and avoid hallucinations.
"""

# !pip install transformers datasets rouge_score tokenizers accelerate torch pandas

import os
import math
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
import logging
import random
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set Device
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using Device: {DEVICE}")

# Set seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# --- 1. Data Ingestion (CNN/DailyMail) ---

print("Downloading CNN/DailyMail Dataset (Version 3.0.0)...")
# This is a large dataset (~300k pairs)
dataset = load_dataset('cnn_dailymail', '3.0.0')

# reduce dataset size slightly if Colab creates issues, but full size is best for quality
# optimize: take 100k for faster training "proof of concept" if needed, but here we use full train
train_data = dataset['train']
val_data = dataset['validation']
test_data = dataset['test']

print(f"Training Samples: {len(train_data)}")
print(f"Validation Samples: {len(val_data)}")

# --- 2. Train BPE Tokenizer ---

if not os.path.exists("data_dump"):
    os.makedirs("data_dump")

# We only use a subset to train tokenizer to save time (10k docs is enough to learn English BPE)
print("preparing tokenizer training data...")
with open("data_dump/train_subset.txt", "w", encoding="utf-8") as f:
    for i in range(min(20000, len(train_data))): 
        t = train_data[i]['article']
        s = train_data[i]['highlights']
        f.write(t + "\n" + s + "\n")

print("Training BPE Tokenizer...")
tokenizer = ByteLevelBPETokenizer()
tokenizer.train(files=["data_dump/train_subset.txt"], vocab_size=30000, min_frequency=2, special_tokens=[
    "<s>", "<pad>", "</s>", "<unk>", "<mask बहन>"
])

if not os.path.exists("tokenizer_large"):
    os.makedirs("tokenizer_large")
tokenizer.save_model("tokenizer_large")

SOS_TOKEN_ID = tokenizer.token_to_id("<s>")
EOS_TOKEN_ID = tokenizer.token_to_id("</s>")
PAD_TOKEN_ID = tokenizer.token_to_id("<pad>")

print(f"Vocab Size: {tokenizer.get_vocab_size()}")

# --- 3. Configuration & Dataset ---

# Hyperparameters
MAX_LEN_TEXT = 512  # Articles are long
MAX_LEN_SUMMARY = 128
BATCH_SIZE = 16    # Smaller batch size because 512 context uses more VRAM
EMB_DIM = 256
N_HEAD = 4
HID_DIM = 512
N_LAYERS = 4
DROPOUT = 0.1
EPOCHS = 10        # 1 epoch on 300k data is equal to 20 epochs on 15k data. 10 epochs is A LOT.
LEARNING_RATE = 0.0001
GRAD_CLIP = 1.0
LABEL_SMOOTHING = 0.1

class CNNDMDataset(Dataset):
    def __init__(self, data, tokenizer, max_len_text, max_len_summary):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len_text = max_len_text
        self.max_len_summary = max_len_summary

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = str(item['article'])
        summary = str(item['highlights'])
        
        # Encode
        enc_src = self.tokenizer.encode(text).ids[:self.max_len_text-2]
        enc_trg = self.tokenizer.encode(summary).ids[:self.max_len_summary-1]
        
        src = enc_src + [PAD_TOKEN_ID] * (self.max_len_text - len(enc_src))
        trg = [SOS_TOKEN_ID] + enc_trg
        trg_y = enc_trg + [EOS_TOKEN_ID]
        
        padding = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg))
        trg = (trg + padding)[:self.max_len_summary]
        
        padding_y = [PAD_TOKEN_ID] * (self.max_len_summary - len(trg_y))
        trg_y = (trg_y + padding_y)[:self.max_len_summary]
        
        return torch.tensor(src), torch.tensor(trg), torch.tensor(trg_y)

print("Creating DataLoaders...")
train_dataset = CNNDMDataset(train_data, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)
val_dataset = CNNDMDataset(val_data, tokenizer, MAX_LEN_TEXT, MAX_LEN_SUMMARY)

# Num workers=2 helps data loading speed
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

# --- 4. Transformer Model ---

class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)
        pos_encoding = torch.zeros(max_len, dim_model)
        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)
        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pos_encoding", pos_encoding)
    def forward(self, token_embedding):
        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])

class TransformerModel(nn.Module):
    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p, pad_idx):
        super().__init__()
        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=2000)
        self.embedding = nn.Embedding(num_tokens, dim_model)
        self.transformer = nn.Transformer(
            d_model=dim_model, nhead=num_heads, num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers, dropout=dropout_p, batch_first=True
        )
        self.out = nn.Linear(dim_model, num_tokens)
        self.pad_idx = pad_idx

    def forward(self, src, trg, src_pad_mask=None, trg_pad_mask=None):
        src_emb = self.embedding(src) * math.sqrt(self.positional_encoder.pos_encoding.shape[2])
        trg_emb = self.embedding(trg) * math.sqrt(self.positional_encoder.pos_encoding.shape[2])
        
        # PE expects [seq, batch, dim]
        src_emb = self.positional_encoder(src_emb.permute(1,0,2)).permute(1,0,2)
        trg_emb = self.positional_encoder(trg_emb.permute(1,0,2)).permute(1,0,2)

        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(DEVICE)
        output = self.transformer(
            src=src_emb, tgt=trg_emb, src_key_padding_mask=src_pad_mask,
            tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask, tgt_mask=trg_mask
        )
        return self.out(output)

    def create_pad_mask(self, matrix, pad_token):
        return (matrix == pad_token)

# --- 5. Training Loop ---

model = TransformerModel(
    num_tokens=tokenizer.get_vocab_size(), dim_model=EMB_DIM, num_heads=N_HEAD, 
    num_encoder_layers=N_LAYERS, num_decoder_layers=N_LAYERS, dropout_p=DROPOUT, pad_idx=PAD_TOKEN_ID
).to(DEVICE)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID, label_smoothing=LABEL_SMOOTHING)

print("Starting Training on CNN/DailyMail...")

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    # Use tqdm only for every 10th batch to save log space on large dataset if needed, or full
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")
    
    for i, (src, trg, trg_y) in enumerate(progress_bar):
        src, trg, trg_y = src.to(DEVICE), trg.to(DEVICE), trg_y.to(DEVICE)
        
        src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
        trg_pad_mask = model.create_pad_mask(trg, PAD_TOKEN_ID)
        
        optimizer.zero_grad()
        output = model(src, trg, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
        
        output = output.reshape(-1, output.shape[-1])
        trg_y = trg_y.reshape(-1)
        
        loss = criterion(output, trg_y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    
    print(f"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}")
    
    # Save every epoch because training is long
    torch.save(model.state_dict(), f"transformer_cnndm_epoch_{epoch+1}.pth")

# --- 6. Inference ---
def beam_search_decode(model, src, tokenizer, max_len=100, beam_size=3):
    model.eval()
    src_pad_mask = model.create_pad_mask(src, PAD_TOKEN_ID)
    beams = [(0, [SOS_TOKEN_ID])] 
    for _ in range(max_len):
        candidates = []
        for score, seq in beams:
            if seq[-1] == EOS_TOKEN_ID:
                candidates.append((score, seq))
                continue
            trg_tensor = torch.tensor([seq]).to(DEVICE)
            trg_pad_mask = model.create_pad_mask(trg_tensor, PAD_TOKEN_ID)
            with torch.no_grad():
                output = model(src, trg_tensor, src_pad_mask=src_pad_mask, trg_pad_mask=trg_pad_mask)
            logits = output[:, -1, :] 
            probs = torch.log_softmax(logits, dim=1)
            topk_probs, topk_indices = torch.topk(probs, beam_size)
            for i in range(beam_size):
                token = topk_indices[0][i].item()
                token_prob = topk_probs[0][i].item()
                candidates.append((score + token_prob, seq + [token]))
        beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_size]
        if all(seq[-1] == EOS_TOKEN_ID for _, seq in beams): break
    return tokenizer.decode(beams[0][1], skip_special_tokens=True)

print("\n--- Testing on one example ---")
src = train_dataset[0][0].unsqueeze(0).to(DEVICE)
print(beam_search_decode(model, src, tokenizer))
